---
title: AI Quantizaion
---



## 왜 이 지식을 탐구하게 되었나
종현이 형의 팟캐스트에서 자주 나온 용어인데 무슨 뜻인지 몰라서 찾아본다.  LoRA가 이것의 일종이라고 하여 전에 찾은 김에 이 지식도 함께 탐구하게 되었다. 

## 무엇인가

**AI 퀀타이제이션(Quantization)**은 딥러닝 모델의 **숫자 표현 방식을 줄여서** 연산 효율성을 높이고 모델을 더 작게 만드는 기술이야. 보통 딥러닝 모델은 **32비트 부동소수점**(FP32) 형식으로 학습되고 실행돼. 이 형식은 높은 정밀도를 제공하지만, 많은 메모리와 계산 자원을 사용해.

**퀀타이제이션**은 이 숫자 표현을 **8비트 정수**(INT8) 같은 더 작은 데이터 형식으로 변환하는 과정을 말해. 이렇게 하면 모델이 사용하는 메모리 양과 연산 속도가 크게 줄어들어서, 특히 **모바일 기기나 엣지 디바이스**처럼 하드웨어 자원이 제한된 환경에서 **실행 성능이 향상**돼. 

### 어떻게 동작하냐면:
1. **정밀도 손실**: [[FP32]]에서 [[INT8]]로 변환할 때, 데이터의 표현 범위가 줄어들어서 약간의 **정밀도 손실**이 생길 수 있어. 하지만 잘 설계된 퀀타이제이션 알고리즘은 이 손실을 최소화해서 모델의 성능을 거의 유지할 수 있어.
2. **모델 크기 축소**: INT8 형식은 FP32보다 데이터 크기가 4배 작기 때문에 모델이 차지하는 메모리 공간이 줄어들어 배포와 실행이 더 간편해져.
3. **연산 속도 향상**: 적은 비트수를 사용하므로 연산이 더 빠르게 수행될 수 있고, 배터리 소모도 줄어드는 효과가 있어.

### 정리하자면:
퀀타이제이션은 **모델의 효율성을 높이기 위한 기술**로, **정밀도를 일부 희생**하면서도 더 **빠른 처리**와 **작은 메모리 사용량**을 가능하게 해. 이 기술은 특히 **AI 응용 프로그램을 실시간으로 구동해야 할 때**나 **리소스가 제한된 기기**에서 많이 사용돼.

모델의 효율을 높이기 위해 파라미터의 개수를 마지막에 줄이는 것이구나 

## 이어지는 호기심

